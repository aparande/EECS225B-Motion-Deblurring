# Motion Deblurring with HDRNet
Final project for EECS225B by Anmol Parande and Dominic Carrano
See results [here](https://docs.google.com/document/d/e/2PACX-1vQBEWDPHiPtSP3cQzW9uWxYx7GTjvaz3hPOCocLVwsjlVgmcf4gDRzNpAjT_gQrq_xuHD02fdGlbSJs/pub)
## Abstract
[HDRNet](https://groups.csail.mit.edu/graphics/hdrnet/data/hdrnet.pdf) is a neural network which uses bilateral grid coefficients to perform real-time corrections, such as contrast enhancement and brightness adjustment, to greatly improve the quality of images taken on cell phones. However, when the system was originally developed, the issue of motion blur—one of the most common problems in cell-phone photography—went unaddressed. Given HDRNet’s success on other tasks, we investigated several ways to extend it to motion deblurring, which would enable a unified framework for cell-phone image enhancement. However, even with extensive modifications such as the use of residual blocks, HDRNet’s bilateral grid model fails to recover the details lost when non-uniform motion blurring occurs.

## Data Generation
All of our models were trained on approximately 1000 images from [BSD500](https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/resources.html). They were generated by via the `blur_data.py` script. This script uses the process described by [*Gong et. Al*](https://donggong1.github.io/docs/blur2mflow_cvpr17.pdf) where for each image, a random motion flow is generated. Each flow vector defines a blur kernel for the corresponding pixel which is then applied to the image.

```
usage: blur_data.py [-h] --data-dir OUTPUT_DIR -n N
```

## Training
Models were trained using `train.ipynb` on Google Colab.

We tried three different loss functions
- MSE
- SSIM (Defined in `metrics.py` with code from [repository](https://github.com/Po-Hsun-Su/pytorch-ssim))
- Perceptual Loss (Defined in `PerceptualLoss.py`)

Models were trained for until convergence (~20 epochs) with Batch Normalization, a `weight_decay` of `1e-8`, and a learning rate of `1e-4` with the ADAM optimizer.

Models can also be trained outside of Colaboratory using the `train.py` script.

```
usage: train.py [-h] [--luma-bins LUMA_BINS]
                [--channel-multiplier CHANNEL_MULTIPLIER]
                [--spatial-bin SPATIAL_BIN] [--batch-norm]
                [--net-input-size NET_INPUT_SIZE]
                [--net-output-size NET_OUTPUT_SIZE]
                [--guide-complexity GUIDE_COMPLEXITY] [--use-residual]
                [--coeff-num COEFF_NUM] [--lr LR]
                [--weight-decay WEIGHT_DECAY] [--batch-size BATCH_SIZE]
                [--epochs EPOCHS] --dataset DATASET [--output-dir OUTPUT_DIR]
                [--resume]
```

## Models
Our trained models can be found in the models folder.

- `hdrnet.pth`: The default HDRNet as described in [Gharbi et. al](https://groups.csail.mit.edu/graphics/hdrnet/data/hdrnet.pdf)
- `model-mse.pth`: Our modified model trained on MSE
- `model-perceptual.pth`: Our modified model trained on Perceptual Loss
- `model-resblock.pth`: Our modified model using Residual Blocks trained on SSIM
- `model-ssim.pth`: Our modified model trained on Perceptual Loss

## Resources Used
- https://github.com/donggong1/motion-flow-syn (MatLab/C code converted to Python for Heterogenous Blurring)
- https://github.com/creotiv/hdrnet-pytorch (PyTorch implementation of HDRNet which we modified)
- https://github.com/Po-Hsun-Su/pytorch-ssim (SSIM Metric)

